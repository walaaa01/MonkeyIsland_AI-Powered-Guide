{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 19.51219512195122,
  "eval_steps": 500,
  "global_step": 200,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0975609756097561,
      "grad_norm": 1.7252811193466187,
      "learning_rate": 4e-05,
      "loss": 3.228,
      "step": 1
    },
    {
      "epoch": 0.1951219512195122,
      "grad_norm": 1.6960481405258179,
      "learning_rate": 8e-05,
      "loss": 3.2893,
      "step": 2
    },
    {
      "epoch": 0.2926829268292683,
      "grad_norm": 1.5444669723510742,
      "learning_rate": 0.00012,
      "loss": 3.3249,
      "step": 3
    },
    {
      "epoch": 0.3902439024390244,
      "grad_norm": 1.6244940757751465,
      "learning_rate": 0.00016,
      "loss": 3.0728,
      "step": 4
    },
    {
      "epoch": 0.4878048780487805,
      "grad_norm": 1.632291316986084,
      "learning_rate": 0.0002,
      "loss": 2.5784,
      "step": 5
    },
    {
      "epoch": 0.5853658536585366,
      "grad_norm": 2.2287161350250244,
      "learning_rate": 0.00019897435897435898,
      "loss": 2.2261,
      "step": 6
    },
    {
      "epoch": 0.6829268292682927,
      "grad_norm": 1.6678283214569092,
      "learning_rate": 0.00019794871794871796,
      "loss": 1.9516,
      "step": 7
    },
    {
      "epoch": 0.7804878048780488,
      "grad_norm": 2.201970338821411,
      "learning_rate": 0.00019692307692307696,
      "loss": 1.6176,
      "step": 8
    },
    {
      "epoch": 0.8780487804878049,
      "grad_norm": 1.6680665016174316,
      "learning_rate": 0.0001958974358974359,
      "loss": 1.5086,
      "step": 9
    },
    {
      "epoch": 0.975609756097561,
      "grad_norm": 1.099625825881958,
      "learning_rate": 0.00019487179487179487,
      "loss": 1.2831,
      "step": 10
    },
    {
      "epoch": 1.0731707317073171,
      "grad_norm": 3.401414394378662,
      "learning_rate": 0.00019384615384615385,
      "loss": 1.6193,
      "step": 11
    },
    {
      "epoch": 1.170731707317073,
      "grad_norm": 2.702592372894287,
      "learning_rate": 0.00019282051282051282,
      "loss": 1.1561,
      "step": 12
    },
    {
      "epoch": 1.2682926829268293,
      "grad_norm": 1.5698528289794922,
      "learning_rate": 0.00019179487179487182,
      "loss": 1.075,
      "step": 13
    },
    {
      "epoch": 1.3658536585365852,
      "grad_norm": 6.127148151397705,
      "learning_rate": 0.0001907692307692308,
      "loss": 1.0486,
      "step": 14
    },
    {
      "epoch": 1.4634146341463414,
      "grad_norm": 1.2195916175842285,
      "learning_rate": 0.00018974358974358974,
      "loss": 0.9713,
      "step": 15
    },
    {
      "epoch": 1.5609756097560976,
      "grad_norm": 1.258501648902893,
      "learning_rate": 0.0001887179487179487,
      "loss": 1.1314,
      "step": 16
    },
    {
      "epoch": 1.6585365853658538,
      "grad_norm": 1.487694501876831,
      "learning_rate": 0.0001876923076923077,
      "loss": 0.9133,
      "step": 17
    },
    {
      "epoch": 1.7560975609756098,
      "grad_norm": 1.4245213270187378,
      "learning_rate": 0.0001866666666666667,
      "loss": 0.7895,
      "step": 18
    },
    {
      "epoch": 1.8536585365853657,
      "grad_norm": 1.5480464696884155,
      "learning_rate": 0.00018564102564102566,
      "loss": 1.0907,
      "step": 19
    },
    {
      "epoch": 1.951219512195122,
      "grad_norm": 1.7398335933685303,
      "learning_rate": 0.00018461538461538463,
      "loss": 0.9968,
      "step": 20
    },
    {
      "epoch": 2.048780487804878,
      "grad_norm": 3.91995906829834,
      "learning_rate": 0.00018358974358974358,
      "loss": 1.4239,
      "step": 21
    },
    {
      "epoch": 2.1463414634146343,
      "grad_norm": 1.47233247756958,
      "learning_rate": 0.00018256410256410258,
      "loss": 0.7174,
      "step": 22
    },
    {
      "epoch": 2.2439024390243905,
      "grad_norm": 1.2097973823547363,
      "learning_rate": 0.00018153846153846155,
      "loss": 0.5948,
      "step": 23
    },
    {
      "epoch": 2.341463414634146,
      "grad_norm": 1.068645715713501,
      "learning_rate": 0.00018051282051282052,
      "loss": 0.7339,
      "step": 24
    },
    {
      "epoch": 2.4390243902439024,
      "grad_norm": 1.0592409372329712,
      "learning_rate": 0.0001794871794871795,
      "loss": 0.6528,
      "step": 25
    },
    {
      "epoch": 2.5365853658536586,
      "grad_norm": 1.0596632957458496,
      "learning_rate": 0.00017846153846153847,
      "loss": 0.8061,
      "step": 26
    },
    {
      "epoch": 2.6341463414634148,
      "grad_norm": 0.9009578227996826,
      "learning_rate": 0.00017743589743589744,
      "loss": 0.629,
      "step": 27
    },
    {
      "epoch": 2.7317073170731705,
      "grad_norm": 1.2286723852157593,
      "learning_rate": 0.00017641025641025642,
      "loss": 0.6742,
      "step": 28
    },
    {
      "epoch": 2.8292682926829267,
      "grad_norm": 1.1690205335617065,
      "learning_rate": 0.0001753846153846154,
      "loss": 0.7333,
      "step": 29
    },
    {
      "epoch": 2.926829268292683,
      "grad_norm": 1.5518940687179565,
      "learning_rate": 0.00017435897435897436,
      "loss": 0.7421,
      "step": 30
    },
    {
      "epoch": 3.024390243902439,
      "grad_norm": 3.3603854179382324,
      "learning_rate": 0.00017333333333333334,
      "loss": 0.7675,
      "step": 31
    },
    {
      "epoch": 3.1219512195121952,
      "grad_norm": 0.9860532879829407,
      "learning_rate": 0.00017230769230769234,
      "loss": 0.3499,
      "step": 32
    },
    {
      "epoch": 3.2195121951219514,
      "grad_norm": 1.2044745683670044,
      "learning_rate": 0.00017128205128205128,
      "loss": 0.3579,
      "step": 33
    },
    {
      "epoch": 3.317073170731707,
      "grad_norm": 1.714441180229187,
      "learning_rate": 0.00017025641025641026,
      "loss": 0.6035,
      "step": 34
    },
    {
      "epoch": 3.4146341463414633,
      "grad_norm": 1.1447982788085938,
      "learning_rate": 0.00016923076923076923,
      "loss": 0.5041,
      "step": 35
    },
    {
      "epoch": 3.5121951219512195,
      "grad_norm": 1.0145052671432495,
      "learning_rate": 0.00016820512820512823,
      "loss": 0.383,
      "step": 36
    },
    {
      "epoch": 3.6097560975609757,
      "grad_norm": 1.2733514308929443,
      "learning_rate": 0.0001671794871794872,
      "loss": 0.5168,
      "step": 37
    },
    {
      "epoch": 3.7073170731707314,
      "grad_norm": 1.1667938232421875,
      "learning_rate": 0.00016615384615384617,
      "loss": 0.5832,
      "step": 38
    },
    {
      "epoch": 3.8048780487804876,
      "grad_norm": 1.195909857749939,
      "learning_rate": 0.00016512820512820512,
      "loss": 0.5005,
      "step": 39
    },
    {
      "epoch": 3.902439024390244,
      "grad_norm": 1.3340129852294922,
      "learning_rate": 0.0001641025641025641,
      "loss": 0.428,
      "step": 40
    },
    {
      "epoch": 4.0,
      "grad_norm": 4.232228755950928,
      "learning_rate": 0.0001630769230769231,
      "loss": 0.6787,
      "step": 41
    },
    {
      "epoch": 4.097560975609756,
      "grad_norm": 1.0194898843765259,
      "learning_rate": 0.00016205128205128207,
      "loss": 0.1964,
      "step": 42
    },
    {
      "epoch": 4.195121951219512,
      "grad_norm": 1.485925555229187,
      "learning_rate": 0.00016102564102564104,
      "loss": 0.351,
      "step": 43
    },
    {
      "epoch": 4.2926829268292686,
      "grad_norm": 1.311352252960205,
      "learning_rate": 0.00016,
      "loss": 0.2656,
      "step": 44
    },
    {
      "epoch": 4.390243902439025,
      "grad_norm": 1.5402047634124756,
      "learning_rate": 0.00015897435897435896,
      "loss": 0.3612,
      "step": 45
    },
    {
      "epoch": 4.487804878048781,
      "grad_norm": 1.2909637689590454,
      "learning_rate": 0.00015794871794871796,
      "loss": 0.3266,
      "step": 46
    },
    {
      "epoch": 4.585365853658536,
      "grad_norm": 1.200832486152649,
      "learning_rate": 0.00015692307692307693,
      "loss": 0.3273,
      "step": 47
    },
    {
      "epoch": 4.682926829268292,
      "grad_norm": 1.1950589418411255,
      "learning_rate": 0.0001558974358974359,
      "loss": 0.2738,
      "step": 48
    },
    {
      "epoch": 4.780487804878049,
      "grad_norm": 1.0292373895645142,
      "learning_rate": 0.00015487179487179488,
      "loss": 0.237,
      "step": 49
    },
    {
      "epoch": 4.878048780487805,
      "grad_norm": 1.128025770187378,
      "learning_rate": 0.00015384615384615385,
      "loss": 0.2856,
      "step": 50
    },
    {
      "epoch": 4.975609756097561,
      "grad_norm": 1.2214548587799072,
      "learning_rate": 0.00015282051282051282,
      "loss": 0.2647,
      "step": 51
    },
    {
      "epoch": 5.073170731707317,
      "grad_norm": 2.8064024448394775,
      "learning_rate": 0.0001517948717948718,
      "loss": 0.4462,
      "step": 52
    },
    {
      "epoch": 5.170731707317073,
      "grad_norm": 0.9820175170898438,
      "learning_rate": 0.00015076923076923077,
      "loss": 0.1857,
      "step": 53
    },
    {
      "epoch": 5.2682926829268295,
      "grad_norm": 0.8322370648384094,
      "learning_rate": 0.00014974358974358974,
      "loss": 0.1475,
      "step": 54
    },
    {
      "epoch": 5.365853658536586,
      "grad_norm": 0.8371815085411072,
      "learning_rate": 0.00014871794871794872,
      "loss": 0.1447,
      "step": 55
    },
    {
      "epoch": 5.463414634146342,
      "grad_norm": 1.2317899465560913,
      "learning_rate": 0.00014769230769230772,
      "loss": 0.1819,
      "step": 56
    },
    {
      "epoch": 5.560975609756097,
      "grad_norm": 1.8481831550598145,
      "learning_rate": 0.00014666666666666666,
      "loss": 0.2002,
      "step": 57
    },
    {
      "epoch": 5.658536585365853,
      "grad_norm": 1.3390268087387085,
      "learning_rate": 0.00014564102564102564,
      "loss": 0.1909,
      "step": 58
    },
    {
      "epoch": 5.7560975609756095,
      "grad_norm": 1.4376535415649414,
      "learning_rate": 0.0001446153846153846,
      "loss": 0.1675,
      "step": 59
    },
    {
      "epoch": 5.853658536585366,
      "grad_norm": 1.3340392112731934,
      "learning_rate": 0.0001435897435897436,
      "loss": 0.2276,
      "step": 60
    },
    {
      "epoch": 5.951219512195122,
      "grad_norm": 1.092026710510254,
      "learning_rate": 0.00014256410256410258,
      "loss": 0.2092,
      "step": 61
    },
    {
      "epoch": 6.048780487804878,
      "grad_norm": 3.33996844291687,
      "learning_rate": 0.00014153846153846156,
      "loss": 0.2573,
      "step": 62
    },
    {
      "epoch": 6.146341463414634,
      "grad_norm": 0.8470320105552673,
      "learning_rate": 0.0001405128205128205,
      "loss": 0.1255,
      "step": 63
    },
    {
      "epoch": 6.2439024390243905,
      "grad_norm": 0.8638203144073486,
      "learning_rate": 0.00013948717948717947,
      "loss": 0.1464,
      "step": 64
    },
    {
      "epoch": 6.341463414634147,
      "grad_norm": 0.6896968483924866,
      "learning_rate": 0.00013846153846153847,
      "loss": 0.1405,
      "step": 65
    },
    {
      "epoch": 6.439024390243903,
      "grad_norm": 0.6135915517807007,
      "learning_rate": 0.00013743589743589745,
      "loss": 0.1084,
      "step": 66
    },
    {
      "epoch": 6.536585365853659,
      "grad_norm": 0.7979825139045715,
      "learning_rate": 0.00013641025641025642,
      "loss": 0.1335,
      "step": 67
    },
    {
      "epoch": 6.634146341463414,
      "grad_norm": 0.8685086369514465,
      "learning_rate": 0.0001353846153846154,
      "loss": 0.1263,
      "step": 68
    },
    {
      "epoch": 6.7317073170731705,
      "grad_norm": 2.2361555099487305,
      "learning_rate": 0.00013435897435897437,
      "loss": 0.1944,
      "step": 69
    },
    {
      "epoch": 6.829268292682927,
      "grad_norm": 0.9694234132766724,
      "learning_rate": 0.00013333333333333334,
      "loss": 0.1451,
      "step": 70
    },
    {
      "epoch": 6.926829268292683,
      "grad_norm": 1.8911741971969604,
      "learning_rate": 0.0001323076923076923,
      "loss": 0.1665,
      "step": 71
    },
    {
      "epoch": 7.024390243902439,
      "grad_norm": 2.5838797092437744,
      "learning_rate": 0.00013128205128205129,
      "loss": 0.2082,
      "step": 72
    },
    {
      "epoch": 7.121951219512195,
      "grad_norm": 1.0499331951141357,
      "learning_rate": 0.00013025641025641026,
      "loss": 0.1055,
      "step": 73
    },
    {
      "epoch": 7.219512195121951,
      "grad_norm": 0.728172242641449,
      "learning_rate": 0.00012923076923076923,
      "loss": 0.0951,
      "step": 74
    },
    {
      "epoch": 7.317073170731708,
      "grad_norm": 0.6040210723876953,
      "learning_rate": 0.00012820512820512823,
      "loss": 0.0951,
      "step": 75
    },
    {
      "epoch": 7.414634146341464,
      "grad_norm": 2.0018675327301025,
      "learning_rate": 0.00012717948717948718,
      "loss": 0.1071,
      "step": 76
    },
    {
      "epoch": 7.512195121951219,
      "grad_norm": 0.7644174695014954,
      "learning_rate": 0.00012615384615384615,
      "loss": 0.1092,
      "step": 77
    },
    {
      "epoch": 7.609756097560975,
      "grad_norm": 0.912432849407196,
      "learning_rate": 0.00012512820512820512,
      "loss": 0.1531,
      "step": 78
    },
    {
      "epoch": 7.7073170731707314,
      "grad_norm": 1.2173348665237427,
      "learning_rate": 0.00012410256410256412,
      "loss": 0.1514,
      "step": 79
    },
    {
      "epoch": 7.804878048780488,
      "grad_norm": 0.5180981159210205,
      "learning_rate": 0.0001230769230769231,
      "loss": 0.1089,
      "step": 80
    },
    {
      "epoch": 7.902439024390244,
      "grad_norm": 0.6738005876541138,
      "learning_rate": 0.00012205128205128207,
      "loss": 0.1013,
      "step": 81
    },
    {
      "epoch": 8.0,
      "grad_norm": 1.431858777999878,
      "learning_rate": 0.00012102564102564103,
      "loss": 0.1945,
      "step": 82
    },
    {
      "epoch": 8.097560975609756,
      "grad_norm": 0.5084713101387024,
      "learning_rate": 0.00012,
      "loss": 0.0934,
      "step": 83
    },
    {
      "epoch": 8.195121951219512,
      "grad_norm": 0.90827876329422,
      "learning_rate": 0.00011897435897435898,
      "loss": 0.0973,
      "step": 84
    },
    {
      "epoch": 8.292682926829269,
      "grad_norm": 0.5271151065826416,
      "learning_rate": 0.00011794871794871796,
      "loss": 0.0984,
      "step": 85
    },
    {
      "epoch": 8.390243902439025,
      "grad_norm": 0.5486236810684204,
      "learning_rate": 0.00011692307692307694,
      "loss": 0.0914,
      "step": 86
    },
    {
      "epoch": 8.487804878048781,
      "grad_norm": 0.8184835314750671,
      "learning_rate": 0.00011589743589743591,
      "loss": 0.1091,
      "step": 87
    },
    {
      "epoch": 8.585365853658537,
      "grad_norm": 0.7533606886863708,
      "learning_rate": 0.00011487179487179487,
      "loss": 0.1039,
      "step": 88
    },
    {
      "epoch": 8.682926829268293,
      "grad_norm": 0.4971495270729065,
      "learning_rate": 0.00011384615384615384,
      "loss": 0.0828,
      "step": 89
    },
    {
      "epoch": 8.78048780487805,
      "grad_norm": 1.130924940109253,
      "learning_rate": 0.00011282051282051283,
      "loss": 0.1246,
      "step": 90
    },
    {
      "epoch": 8.878048780487806,
      "grad_norm": 0.8534867763519287,
      "learning_rate": 0.0001117948717948718,
      "loss": 0.1111,
      "step": 91
    },
    {
      "epoch": 8.975609756097562,
      "grad_norm": 1.2395949363708496,
      "learning_rate": 0.00011076923076923077,
      "loss": 0.1035,
      "step": 92
    },
    {
      "epoch": 9.073170731707316,
      "grad_norm": 1.108510136604309,
      "learning_rate": 0.00010974358974358976,
      "loss": 0.1342,
      "step": 93
    },
    {
      "epoch": 9.170731707317072,
      "grad_norm": 0.534378170967102,
      "learning_rate": 0.00010871794871794872,
      "loss": 0.0804,
      "step": 94
    },
    {
      "epoch": 9.268292682926829,
      "grad_norm": 0.43221190571784973,
      "learning_rate": 0.0001076923076923077,
      "loss": 0.0704,
      "step": 95
    },
    {
      "epoch": 9.365853658536585,
      "grad_norm": 0.738764226436615,
      "learning_rate": 0.00010666666666666667,
      "loss": 0.1108,
      "step": 96
    },
    {
      "epoch": 9.463414634146341,
      "grad_norm": 0.4756898581981659,
      "learning_rate": 0.00010564102564102565,
      "loss": 0.084,
      "step": 97
    },
    {
      "epoch": 9.560975609756097,
      "grad_norm": 0.6202285885810852,
      "learning_rate": 0.00010461538461538463,
      "loss": 0.0905,
      "step": 98
    },
    {
      "epoch": 9.658536585365853,
      "grad_norm": 0.5211235880851746,
      "learning_rate": 0.0001035897435897436,
      "loss": 0.0963,
      "step": 99
    },
    {
      "epoch": 9.75609756097561,
      "grad_norm": 0.6050599217414856,
      "learning_rate": 0.00010256410256410256,
      "loss": 0.1094,
      "step": 100
    },
    {
      "epoch": 9.853658536585366,
      "grad_norm": 0.4761507213115692,
      "learning_rate": 0.00010153846153846153,
      "loss": 0.0891,
      "step": 101
    },
    {
      "epoch": 9.951219512195122,
      "grad_norm": 0.5057691335678101,
      "learning_rate": 0.00010051282051282052,
      "loss": 0.1038,
      "step": 102
    },
    {
      "epoch": 10.048780487804878,
      "grad_norm": 0.8977339863777161,
      "learning_rate": 9.948717948717949e-05,
      "loss": 0.1639,
      "step": 103
    },
    {
      "epoch": 10.146341463414634,
      "grad_norm": 0.33879584074020386,
      "learning_rate": 9.846153846153848e-05,
      "loss": 0.0769,
      "step": 104
    },
    {
      "epoch": 10.24390243902439,
      "grad_norm": 0.4050935208797455,
      "learning_rate": 9.743589743589744e-05,
      "loss": 0.0785,
      "step": 105
    },
    {
      "epoch": 10.341463414634147,
      "grad_norm": 0.2509424388408661,
      "learning_rate": 9.641025641025641e-05,
      "loss": 0.0716,
      "step": 106
    },
    {
      "epoch": 10.439024390243903,
      "grad_norm": 0.2622378170490265,
      "learning_rate": 9.53846153846154e-05,
      "loss": 0.0727,
      "step": 107
    },
    {
      "epoch": 10.536585365853659,
      "grad_norm": 0.2667994201183319,
      "learning_rate": 9.435897435897436e-05,
      "loss": 0.0781,
      "step": 108
    },
    {
      "epoch": 10.634146341463415,
      "grad_norm": 0.32511502504348755,
      "learning_rate": 9.333333333333334e-05,
      "loss": 0.0808,
      "step": 109
    },
    {
      "epoch": 10.731707317073171,
      "grad_norm": 0.3450073301792145,
      "learning_rate": 9.230769230769232e-05,
      "loss": 0.0821,
      "step": 110
    },
    {
      "epoch": 10.829268292682928,
      "grad_norm": 0.43131721019744873,
      "learning_rate": 9.128205128205129e-05,
      "loss": 0.0868,
      "step": 111
    },
    {
      "epoch": 10.926829268292684,
      "grad_norm": 0.44079628586769104,
      "learning_rate": 9.025641025641026e-05,
      "loss": 0.0858,
      "step": 112
    },
    {
      "epoch": 11.024390243902438,
      "grad_norm": 1.566673755645752,
      "learning_rate": 8.923076923076924e-05,
      "loss": 0.1507,
      "step": 113
    },
    {
      "epoch": 11.121951219512194,
      "grad_norm": 0.31134289503097534,
      "learning_rate": 8.820512820512821e-05,
      "loss": 0.0719,
      "step": 114
    },
    {
      "epoch": 11.21951219512195,
      "grad_norm": 0.3450903296470642,
      "learning_rate": 8.717948717948718e-05,
      "loss": 0.0667,
      "step": 115
    },
    {
      "epoch": 11.317073170731707,
      "grad_norm": 0.39986127614974976,
      "learning_rate": 8.615384615384617e-05,
      "loss": 0.072,
      "step": 116
    },
    {
      "epoch": 11.414634146341463,
      "grad_norm": 0.36912983655929565,
      "learning_rate": 8.512820512820513e-05,
      "loss": 0.079,
      "step": 117
    },
    {
      "epoch": 11.512195121951219,
      "grad_norm": 0.5324628949165344,
      "learning_rate": 8.410256410256411e-05,
      "loss": 0.0814,
      "step": 118
    },
    {
      "epoch": 11.609756097560975,
      "grad_norm": 0.5872491002082825,
      "learning_rate": 8.307692307692309e-05,
      "loss": 0.0868,
      "step": 119
    },
    {
      "epoch": 11.707317073170731,
      "grad_norm": 0.5929949283599854,
      "learning_rate": 8.205128205128205e-05,
      "loss": 0.0792,
      "step": 120
    },
    {
      "epoch": 11.804878048780488,
      "grad_norm": 0.29134416580200195,
      "learning_rate": 8.102564102564103e-05,
      "loss": 0.0722,
      "step": 121
    },
    {
      "epoch": 11.902439024390244,
      "grad_norm": 0.4795916974544525,
      "learning_rate": 8e-05,
      "loss": 0.0996,
      "step": 122
    },
    {
      "epoch": 12.0,
      "grad_norm": 1.0791230201721191,
      "learning_rate": 7.897435897435898e-05,
      "loss": 0.1499,
      "step": 123
    },
    {
      "epoch": 12.097560975609756,
      "grad_norm": 0.29571372270584106,
      "learning_rate": 7.794871794871795e-05,
      "loss": 0.0725,
      "step": 124
    },
    {
      "epoch": 12.195121951219512,
      "grad_norm": 0.3886522352695465,
      "learning_rate": 7.692307692307693e-05,
      "loss": 0.08,
      "step": 125
    },
    {
      "epoch": 12.292682926829269,
      "grad_norm": 0.4859776198863983,
      "learning_rate": 7.58974358974359e-05,
      "loss": 0.084,
      "step": 126
    },
    {
      "epoch": 12.390243902439025,
      "grad_norm": 0.2525789141654968,
      "learning_rate": 7.487179487179487e-05,
      "loss": 0.0672,
      "step": 127
    },
    {
      "epoch": 12.487804878048781,
      "grad_norm": 0.2839777171611786,
      "learning_rate": 7.384615384615386e-05,
      "loss": 0.0712,
      "step": 128
    },
    {
      "epoch": 12.585365853658537,
      "grad_norm": 0.2630588114261627,
      "learning_rate": 7.282051282051282e-05,
      "loss": 0.0727,
      "step": 129
    },
    {
      "epoch": 12.682926829268293,
      "grad_norm": 0.299175500869751,
      "learning_rate": 7.17948717948718e-05,
      "loss": 0.0729,
      "step": 130
    },
    {
      "epoch": 12.78048780487805,
      "grad_norm": 0.41170749068260193,
      "learning_rate": 7.076923076923078e-05,
      "loss": 0.0992,
      "step": 131
    },
    {
      "epoch": 12.878048780487806,
      "grad_norm": 0.2996871769428253,
      "learning_rate": 6.974358974358974e-05,
      "loss": 0.0804,
      "step": 132
    },
    {
      "epoch": 12.975609756097562,
      "grad_norm": 0.26537829637527466,
      "learning_rate": 6.871794871794872e-05,
      "loss": 0.0706,
      "step": 133
    },
    {
      "epoch": 13.073170731707316,
      "grad_norm": 0.7599324584007263,
      "learning_rate": 6.76923076923077e-05,
      "loss": 0.1278,
      "step": 134
    },
    {
      "epoch": 13.170731707317072,
      "grad_norm": 0.2852095663547516,
      "learning_rate": 6.666666666666667e-05,
      "loss": 0.0763,
      "step": 135
    },
    {
      "epoch": 13.268292682926829,
      "grad_norm": 0.27526745200157166,
      "learning_rate": 6.564102564102564e-05,
      "loss": 0.0719,
      "step": 136
    },
    {
      "epoch": 13.365853658536585,
      "grad_norm": 0.24405179917812347,
      "learning_rate": 6.461538461538462e-05,
      "loss": 0.0693,
      "step": 137
    },
    {
      "epoch": 13.463414634146341,
      "grad_norm": 0.35884714126586914,
      "learning_rate": 6.358974358974359e-05,
      "loss": 0.0859,
      "step": 138
    },
    {
      "epoch": 13.560975609756097,
      "grad_norm": 0.24423493444919586,
      "learning_rate": 6.256410256410256e-05,
      "loss": 0.0701,
      "step": 139
    },
    {
      "epoch": 13.658536585365853,
      "grad_norm": 0.3497677147388458,
      "learning_rate": 6.153846153846155e-05,
      "loss": 0.0738,
      "step": 140
    },
    {
      "epoch": 13.75609756097561,
      "grad_norm": 0.2341243326663971,
      "learning_rate": 6.0512820512820515e-05,
      "loss": 0.0598,
      "step": 141
    },
    {
      "epoch": 13.853658536585366,
      "grad_norm": 0.41114139556884766,
      "learning_rate": 5.948717948717949e-05,
      "loss": 0.0901,
      "step": 142
    },
    {
      "epoch": 13.951219512195122,
      "grad_norm": 0.3045092821121216,
      "learning_rate": 5.846153846153847e-05,
      "loss": 0.078,
      "step": 143
    },
    {
      "epoch": 14.048780487804878,
      "grad_norm": 0.557181179523468,
      "learning_rate": 5.7435897435897434e-05,
      "loss": 0.1225,
      "step": 144
    },
    {
      "epoch": 14.146341463414634,
      "grad_norm": 0.2656087875366211,
      "learning_rate": 5.6410256410256414e-05,
      "loss": 0.0651,
      "step": 145
    },
    {
      "epoch": 14.24390243902439,
      "grad_norm": 0.33594387769699097,
      "learning_rate": 5.538461538461539e-05,
      "loss": 0.0785,
      "step": 146
    },
    {
      "epoch": 14.341463414634147,
      "grad_norm": 0.35009390115737915,
      "learning_rate": 5.435897435897436e-05,
      "loss": 0.0738,
      "step": 147
    },
    {
      "epoch": 14.439024390243903,
      "grad_norm": 0.2908076047897339,
      "learning_rate": 5.333333333333333e-05,
      "loss": 0.0758,
      "step": 148
    },
    {
      "epoch": 14.536585365853659,
      "grad_norm": 0.32644015550613403,
      "learning_rate": 5.230769230769231e-05,
      "loss": 0.0675,
      "step": 149
    },
    {
      "epoch": 14.634146341463415,
      "grad_norm": 0.31920820474624634,
      "learning_rate": 5.128205128205128e-05,
      "loss": 0.0743,
      "step": 150
    },
    {
      "epoch": 14.731707317073171,
      "grad_norm": 0.24752578139305115,
      "learning_rate": 5.025641025641026e-05,
      "loss": 0.0637,
      "step": 151
    },
    {
      "epoch": 14.829268292682928,
      "grad_norm": 0.28421515226364136,
      "learning_rate": 4.923076923076924e-05,
      "loss": 0.0722,
      "step": 152
    },
    {
      "epoch": 14.926829268292684,
      "grad_norm": 0.3711795508861542,
      "learning_rate": 4.8205128205128205e-05,
      "loss": 0.0789,
      "step": 153
    },
    {
      "epoch": 15.024390243902438,
      "grad_norm": 1.070664644241333,
      "learning_rate": 4.717948717948718e-05,
      "loss": 0.1275,
      "step": 154
    },
    {
      "epoch": 15.121951219512194,
      "grad_norm": 0.2777349352836609,
      "learning_rate": 4.615384615384616e-05,
      "loss": 0.0681,
      "step": 155
    },
    {
      "epoch": 15.21951219512195,
      "grad_norm": 0.32634881138801575,
      "learning_rate": 4.512820512820513e-05,
      "loss": 0.0695,
      "step": 156
    },
    {
      "epoch": 15.317073170731707,
      "grad_norm": 0.31470784544944763,
      "learning_rate": 4.4102564102564104e-05,
      "loss": 0.0702,
      "step": 157
    },
    {
      "epoch": 15.414634146341463,
      "grad_norm": 0.24409028887748718,
      "learning_rate": 4.3076923076923084e-05,
      "loss": 0.0685,
      "step": 158
    },
    {
      "epoch": 15.512195121951219,
      "grad_norm": 0.3380560576915741,
      "learning_rate": 4.205128205128206e-05,
      "loss": 0.0735,
      "step": 159
    },
    {
      "epoch": 15.609756097560975,
      "grad_norm": 0.4108578860759735,
      "learning_rate": 4.1025641025641023e-05,
      "loss": 0.074,
      "step": 160
    },
    {
      "epoch": 15.707317073170731,
      "grad_norm": 0.22406618297100067,
      "learning_rate": 4e-05,
      "loss": 0.069,
      "step": 161
    },
    {
      "epoch": 15.804878048780488,
      "grad_norm": 0.3529069721698761,
      "learning_rate": 3.8974358974358976e-05,
      "loss": 0.0695,
      "step": 162
    },
    {
      "epoch": 15.902439024390244,
      "grad_norm": 0.34290605783462524,
      "learning_rate": 3.794871794871795e-05,
      "loss": 0.08,
      "step": 163
    },
    {
      "epoch": 16.0,
      "grad_norm": 0.6230884194374084,
      "learning_rate": 3.692307692307693e-05,
      "loss": 0.116,
      "step": 164
    },
    {
      "epoch": 16.097560975609756,
      "grad_norm": 0.2845827341079712,
      "learning_rate": 3.58974358974359e-05,
      "loss": 0.0671,
      "step": 165
    },
    {
      "epoch": 16.195121951219512,
      "grad_norm": 0.39164090156555176,
      "learning_rate": 3.487179487179487e-05,
      "loss": 0.0778,
      "step": 166
    },
    {
      "epoch": 16.29268292682927,
      "grad_norm": 0.30605828762054443,
      "learning_rate": 3.384615384615385e-05,
      "loss": 0.0668,
      "step": 167
    },
    {
      "epoch": 16.390243902439025,
      "grad_norm": 0.2787618339061737,
      "learning_rate": 3.282051282051282e-05,
      "loss": 0.0739,
      "step": 168
    },
    {
      "epoch": 16.48780487804878,
      "grad_norm": 0.2560344338417053,
      "learning_rate": 3.1794871794871795e-05,
      "loss": 0.0658,
      "step": 169
    },
    {
      "epoch": 16.585365853658537,
      "grad_norm": 0.3633984625339508,
      "learning_rate": 3.0769230769230774e-05,
      "loss": 0.0674,
      "step": 170
    },
    {
      "epoch": 16.682926829268293,
      "grad_norm": 0.35520654916763306,
      "learning_rate": 2.9743589743589744e-05,
      "loss": 0.0738,
      "step": 171
    },
    {
      "epoch": 16.78048780487805,
      "grad_norm": 0.25012028217315674,
      "learning_rate": 2.8717948717948717e-05,
      "loss": 0.0674,
      "step": 172
    },
    {
      "epoch": 16.878048780487806,
      "grad_norm": 0.3188059628009796,
      "learning_rate": 2.7692307692307694e-05,
      "loss": 0.0686,
      "step": 173
    },
    {
      "epoch": 16.975609756097562,
      "grad_norm": 0.3276621997356415,
      "learning_rate": 2.6666666666666667e-05,
      "loss": 0.0704,
      "step": 174
    },
    {
      "epoch": 17.073170731707318,
      "grad_norm": 0.8295034170150757,
      "learning_rate": 2.564102564102564e-05,
      "loss": 0.1085,
      "step": 175
    },
    {
      "epoch": 17.170731707317074,
      "grad_norm": 0.25870412588119507,
      "learning_rate": 2.461538461538462e-05,
      "loss": 0.0676,
      "step": 176
    },
    {
      "epoch": 17.26829268292683,
      "grad_norm": 0.3022550642490387,
      "learning_rate": 2.358974358974359e-05,
      "loss": 0.0763,
      "step": 177
    },
    {
      "epoch": 17.365853658536587,
      "grad_norm": 0.24658511579036713,
      "learning_rate": 2.2564102564102566e-05,
      "loss": 0.0614,
      "step": 178
    },
    {
      "epoch": 17.463414634146343,
      "grad_norm": 0.26651740074157715,
      "learning_rate": 2.1538461538461542e-05,
      "loss": 0.0606,
      "step": 179
    },
    {
      "epoch": 17.5609756097561,
      "grad_norm": 0.2799442708492279,
      "learning_rate": 2.0512820512820512e-05,
      "loss": 0.0665,
      "step": 180
    },
    {
      "epoch": 17.658536585365855,
      "grad_norm": 0.33093029260635376,
      "learning_rate": 1.9487179487179488e-05,
      "loss": 0.0703,
      "step": 181
    },
    {
      "epoch": 17.75609756097561,
      "grad_norm": 0.32196828722953796,
      "learning_rate": 1.8461538461538465e-05,
      "loss": 0.0676,
      "step": 182
    },
    {
      "epoch": 17.853658536585368,
      "grad_norm": 0.37666386365890503,
      "learning_rate": 1.7435897435897434e-05,
      "loss": 0.0723,
      "step": 183
    },
    {
      "epoch": 17.951219512195124,
      "grad_norm": 0.375476211309433,
      "learning_rate": 1.641025641025641e-05,
      "loss": 0.0693,
      "step": 184
    },
    {
      "epoch": 18.048780487804876,
      "grad_norm": 0.7945569157600403,
      "learning_rate": 1.5384615384615387e-05,
      "loss": 0.1165,
      "step": 185
    },
    {
      "epoch": 18.146341463414632,
      "grad_norm": 0.25475987792015076,
      "learning_rate": 1.4358974358974359e-05,
      "loss": 0.064,
      "step": 186
    },
    {
      "epoch": 18.24390243902439,
      "grad_norm": 0.30152079463005066,
      "learning_rate": 1.3333333333333333e-05,
      "loss": 0.0661,
      "step": 187
    },
    {
      "epoch": 18.341463414634145,
      "grad_norm": 0.2332487851381302,
      "learning_rate": 1.230769230769231e-05,
      "loss": 0.0665,
      "step": 188
    },
    {
      "epoch": 18.4390243902439,
      "grad_norm": 0.3219779133796692,
      "learning_rate": 1.1282051282051283e-05,
      "loss": 0.0662,
      "step": 189
    },
    {
      "epoch": 18.536585365853657,
      "grad_norm": 0.2937256395816803,
      "learning_rate": 1.0256410256410256e-05,
      "loss": 0.0689,
      "step": 190
    },
    {
      "epoch": 18.634146341463413,
      "grad_norm": 0.3193691074848175,
      "learning_rate": 9.230769230769232e-06,
      "loss": 0.0681,
      "step": 191
    },
    {
      "epoch": 18.73170731707317,
      "grad_norm": 0.34001728892326355,
      "learning_rate": 8.205128205128205e-06,
      "loss": 0.0693,
      "step": 192
    },
    {
      "epoch": 18.829268292682926,
      "grad_norm": 0.3232141435146332,
      "learning_rate": 7.179487179487179e-06,
      "loss": 0.069,
      "step": 193
    },
    {
      "epoch": 18.926829268292682,
      "grad_norm": 0.3063494861125946,
      "learning_rate": 6.153846153846155e-06,
      "loss": 0.0651,
      "step": 194
    },
    {
      "epoch": 19.024390243902438,
      "grad_norm": 0.73753422498703,
      "learning_rate": 5.128205128205128e-06,
      "loss": 0.1162,
      "step": 195
    },
    {
      "epoch": 19.121951219512194,
      "grad_norm": 0.29308491945266724,
      "learning_rate": 4.102564102564103e-06,
      "loss": 0.0665,
      "step": 196
    },
    {
      "epoch": 19.21951219512195,
      "grad_norm": 0.2817195951938629,
      "learning_rate": 3.0769230769230774e-06,
      "loss": 0.0688,
      "step": 197
    },
    {
      "epoch": 19.317073170731707,
      "grad_norm": 0.2621106803417206,
      "learning_rate": 2.0512820512820513e-06,
      "loss": 0.0653,
      "step": 198
    },
    {
      "epoch": 19.414634146341463,
      "grad_norm": 0.2592458426952362,
      "learning_rate": 1.0256410256410257e-06,
      "loss": 0.0653,
      "step": 199
    },
    {
      "epoch": 19.51219512195122,
      "grad_norm": 0.314328134059906,
      "learning_rate": 0.0,
      "loss": 0.0664,
      "step": 200
    }
  ],
  "logging_steps": 1,
  "max_steps": 200,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 20,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 5305236689240064.0,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
